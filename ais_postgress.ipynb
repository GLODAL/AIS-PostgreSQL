{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45886093-9cb9-4896-a080-a43672c1143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd, sqlalchemy\n",
    "from datetime import datetime\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89cc3d80-a3a9-439a-a5a2-0d262463ae0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine(postgresql+psycopg2://appuser:***@pg-postgresql.jrycastillo.svc:5432/appdb)\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(\n",
    "    \"postgresql+psycopg2://appuser:$Q59s78u@pg-postgresql.jrycastillo.svc:5432/appdb\"\n",
    ")\n",
    "print(engine)  # should show postgresql+psycopg2, NOT sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3011fd7d-483f-49cd-b444-fcfaedff5081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql+psycopg2://appuser:***@pg-postgresql.jrycastillo.svc:5432/appdb\n"
     ]
    }
   ],
   "source": [
    "print(engine.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b96e220-9fdf-4efe-a1b9-c01e7fbc99cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 2.1.4\n",
      "sqlalchemy: 2.0.15\n"
     ]
    }
   ],
   "source": [
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"sqlalchemy:\", sqlalchemy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da216189-7264-4e67-bc1e-3e5a00305125",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2019, 2026):\n",
    "    table_name = f\"ais_mtable_y{year}\"\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        created_at TIMESTAMP NOT NULL,\n",
    "        ts TIMESTAMP,\n",
    "        static_timestamp TEXT,\n",
    "        prev_mmsi TEXT,\n",
    "        latitude DOUBLE PRECISION,\n",
    "        longitude DOUBLE PRECISION,\n",
    "        speed DOUBLE PRECISION,\n",
    "        course DOUBLE PRECISION,\n",
    "        heading DOUBLE PRECISION,\n",
    "        imo TEXT,\n",
    "        name TEXT,\n",
    "        call_sign TEXT,\n",
    "        flag TEXT,\n",
    "        draught TEXT,\n",
    "        ship_and_cargo_type TEXT,\n",
    "        ship_type TEXT,\n",
    "        length TEXT,\n",
    "        width TEXT,\n",
    "        eta TEXT,\n",
    "        destination TEXT,\n",
    "        status TEXT,\n",
    "        maneuver TEXT,\n",
    "        accuracy BIGINT,\n",
    "        rot TEXT,\n",
    "        collection_type TEXT,\n",
    "        mmsi TEXT\n",
    "    ) PARTITION BY RANGE (created_at);\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn: \n",
    "            conn.execute(text(create_table_sql))\n",
    "            print(f\"Parent table '{table_name}' created (or already exists).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create table '{table_name}':\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef0132-9690-4508-8606-5c2af66e1835",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2019, 2026):\n",
    "    parent_table = f\"ais_mtable_y{year}\"\n",
    "    \n",
    "    for month in range(1, 13):\n",
    "        start_date = datetime(year, month, 1)\n",
    "        if month == 12:\n",
    "            end_date = datetime(year + 1, 1, 1)\n",
    "        else:\n",
    "            end_date = datetime(year, month + 1, 1)\n",
    "\n",
    "        partition_name = f\"{parent_table}_p{month:02d}\"\n",
    "        create_partition_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {partition_name} PARTITION OF {parent_table}\n",
    "        FOR VALUES FROM ('{start_date.strftime('%Y-%m-%d')}') TO ('{end_date.strftime('%Y-%m-%d')}');\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            with engine.begin() as conn: \n",
    "                conn.execute(text(create_partition_sql))\n",
    "                print(f\"✅ Created partition: {partition_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create partition '{partition_name}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41c2950-437e-4b1a-a063-0721c0f7b6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8a205-4c3a-4a72-99bb-81a0cd900ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cfbf3f-84a7-45d5-adc5-42c260d9dcf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506daf6f-e649-46fd-917f-3e11d05e7f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9008b2fa-e876-42c9-81a7-eba08bdde620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def format_time(seconds):\n",
    "    seconds = int(seconds)\n",
    "    hrs, rem = divmod(seconds, 3600)\n",
    "    mins, secs = divmod(rem, 60)\n",
    "    time_str = \"\"\n",
    "    if hrs > 0:\n",
    "        time_str += f\"{hrs} hr \"\n",
    "    if mins > 0 or hrs > 0:\n",
    "        time_str += f\"{mins} min \"\n",
    "    time_str += f\"{secs} sec\"\n",
    "    return time_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70441d8e-f8dd-4fe1-8eb1-a73c03a747f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_log_file = \"upload_log.csv\"\n",
    "\n",
    "if not os.path.exists(upload_log_file):\n",
    "    pd.DataFrame(columns=[\n",
    "        \"filename\",      \n",
    "        \"unique_years\",    \n",
    "        \"time\",           \n",
    "        \"rows\",          \n",
    "        \"uploaded_datetime\"\n",
    "    ]).to_csv(upload_log_file, index=False)\n",
    "    print(f\"🆕 Created log file: {upload_log_file}\")\n",
    "\n",
    "upload_log = pd.read_csv(upload_log_file, parse_dates=[\"uploaded_datetime\"])\n",
    "print(f\"📖 Loaded log file: {upload_log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d2059-a3cd-4546-8a60-97dd413235ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_folder = \"/home/jovyan/shared/val/task/250728-LM-AIS_2022-2024_cleaned\"\n",
    "parquet_files = [f for f in os.listdir(parquet_folder) if f.endswith(\".parquet\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7521611-ba0c-4b35-9337-bce1da10bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_copybuf(df: pd.DataFrame) -> io.StringIO:\n",
    "    buf = io.StringIO()\n",
    "    df.to_csv(buf, index=False, header=False) \n",
    "    buf.seek(0)\n",
    "    return buf\n",
    "\n",
    "\n",
    "BATCH_ROWS = 200_000\n",
    "\n",
    "for file in parquet_files:\n",
    "    if file in upload_log['filename'].values:\n",
    "        print(f\"Skipping {file}, already uploaded.\")\n",
    "        continue\n",
    "\n",
    "    parquet_path = os.path.join(parquet_folder, file)\n",
    "    print(f\"Processing {file}...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    df = pd.read_parquet(parquet_path, engine=\"pyarrow\")\n",
    "    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"])\n",
    "    unique_years_list = sorted(df[\"created_at\"].dt.year.unique().tolist())\n",
    "\n",
    "    total_rows = 0\n",
    "    print(\"UNIQUE YEARS:\", unique_years_list)\n",
    "    for year in unique_years_list:\n",
    "        table_name = f\"ais_mtable_y{year}\"\n",
    "        print(f\"Uploading to table: {table_name}\")\n",
    "        df_year = df[df[\"created_at\"].dt.year == year]\n",
    "\n",
    "        rows_count = len(df_year)\n",
    "        if rows_count == 0:\n",
    "            continue\n",
    "\n",
    "        cols = \",\".join([f'\"{c}\"' for c in df_year.columns])\n",
    "        copy_sql = f'COPY {table_name} ({cols}) FROM STDIN WITH CSV'\n",
    "\n",
    "        conn = engine.raw_connection()\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        steps = (rows_count // BATCH_ROWS) + 1\n",
    "        for i in range(steps):\n",
    "            chunk = df_year.iloc[i*BATCH_ROWS : (i+1)*BATCH_ROWS]\n",
    "            if len(chunk) == 0:\n",
    "                continue\n",
    "            buf = df_to_copybuf(chunk)\n",
    "            cur.copy_expert(copy_sql, buf)\n",
    "            print(f\"   -> batch {i+1}/{steps}, rows={len(chunk)}\")\n",
    "\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "        total_rows += rows_count\n",
    "        print(f\"Inserted {rows_count} rows for year {year}\")\n",
    "\n",
    "    elapsed_time_seconds = time.time() - start_time\n",
    "    elapsed_time = format_time(elapsed_time_seconds)\n",
    "    uploaded_datetime = datetime.now()\n",
    "\n",
    "    upload_log = pd.concat([upload_log, pd.DataFrame({\n",
    "        \"filename\": [file],\n",
    "        \"unique_years\": [unique_years_list],\n",
    "        \"time\": [elapsed_time],\n",
    "        \"rows\": [total_rows],\n",
    "        \"uploaded_datetime\": [uploaded_datetime]\n",
    "    })], ignore_index=True)\n",
    "    upload_log.to_csv(upload_log_file, index=False)\n",
    "\n",
    "    print(f\"Finished uploading {file} in {elapsed_time}, total rows={total_rows}.\\n\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c6e51f-f981-48b9-9117-1e02d8f235bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f088666e-dac1-4c24-8b48-16407f7706c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b002cb-6b70-4620-9bf5-1fd35cf61ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbb5146-1faf-4200-84e7-20caa9189c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "TABLE = \"public.ais_mtable_y2024_p02\" \n",
    "DATE_COLUMN = \"created_at\"            \n",
    "CHUNKSIZE = 2_000_000                   \n",
    "BASE_DIR = \"output_dl_postgres\"         \n",
    "COMPRESSION = \"snappy\"                \n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = os.path.join(BASE_DIR, f\"{TABLE.replace('.', '_')}_{timestamp}\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Saving Parquet files to: {output_dir}\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(f\"SELECT MIN({DATE_COLUMN}), MAX({DATE_COLUMN}) FROM {TABLE}\"))\n",
    "    min_date, max_date = result.fetchone()\n",
    "\n",
    "min_date = pd.Timestamp(min_date).normalize()\n",
    "max_date = pd.Timestamp(max_date).normalize()\n",
    "print(f\"Date range: {min_date} → {max_date}\")\n",
    "\n",
    "current = min_date\n",
    "day_idx = 0\n",
    "\n",
    "while current <= max_date:\n",
    "    next_day = current + timedelta(days=1)\n",
    "    day_idx += 1\n",
    "    print(f\"\\n📅 Processing day {day_idx}: {current.date()} → {next_day.date()}\")\n",
    "\n",
    "    raw_conn = engine.raw_connection()\n",
    "    cur = raw_conn.cursor(name='stream_cursor')\n",
    "\n",
    "    cur.execute(f\"\"\"\n",
    "        SELECT * FROM {TABLE}\n",
    "        WHERE {DATE_COLUMN} >= %s AND {DATE_COLUMN} < %s\n",
    "    \"\"\", (current, next_day))\n",
    "\n",
    "    chunk_count = 0\n",
    "    total_rows_day = 0\n",
    "\n",
    "    while True:\n",
    "        rows = cur.fetchmany(CHUNKSIZE)\n",
    "        if not rows:\n",
    "            break\n",
    "\n",
    "        df_chunk = pd.DataFrame(rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "        file_name = f\"{TABLE.replace('.', '_')}_{current.date()}_{chunk_count}.parquet\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "        df_chunk.to_parquet(file_path, index=False, engine=\"pyarrow\", compression=COMPRESSION)\n",
    "        chunk_count += 1\n",
    "        total_rows_day += len(df_chunk)\n",
    "        print(f\"   ✅ Chunk {chunk_count} saved → {file_name} ({len(df_chunk)} rows)\")\n",
    "\n",
    "    if chunk_count == 0:\n",
    "        print(f\"   ⚠️ No rows found for {current.date()}\")\n",
    "    else:\n",
    "        print(f\"   📊 Finished {current.date()} → {chunk_count} chunks, {total_rows_day} rows\")\n",
    "\n",
    "    cur.close()\n",
    "    raw_conn.close()\n",
    "    current = next_day\n",
    "\n",
    "print(\"\\n🎉 Export complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
